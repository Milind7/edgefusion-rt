{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dncWnH7ZxM3G",
        "outputId": "b7534d9c-70f5-4cfa-fc79-f89f88c5df03"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Milind7/edgefusion-rt.git\n",
        "!pip install torch torchvision onnx onnxruntime rich"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbBSO7BFy0lo"
      },
      "source": [
        "| Device Type | PyTorch device string | How to detect                                                 | Typical hardware             |\n",
        "| ----------- | --------------------- | ------------------------------------------------------------- | ---------------------------- |\n",
        "| CPU         | `\"cpu\"`               | Always available (`torch.device(\"cpu\")`)                      | Intel/AMD cores              |\n",
        "| GPU (CUDA)  | `\"cuda\"` / `\"cuda:0\"` | `torch.cuda.is_available()` + `torch.cuda.get_device_name(0)` | Tesla K80 / T4 / P100 / V100 |\n",
        "| Apple MPS   | `\"mps\"`               | `torch.backends.mps.is_available()`                           | Apple M1/M2 GPUs             |\n",
        "| TPU         | via `torch_xla`       | `os.environ.get(\"COLAB_TPU_ADDR\")`                            | Google TPU v2/v3             |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eruHCcKnxSrm",
        "outputId": "327ad063-2eb2-461c-bfcb-bcb8e6b60578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "  Device 0: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"  Device {i}: {torch.cuda.get_device_name(i)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnPr1anGyMqC",
        "outputId": "e7cc694f-6883-444f-a5b1-3e02b8a66148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;32mFP32 latency:\u001b[0m \u001b[1;36m5.13\u001b[0m ms on CUDA\n"
          ]
        }
      ],
      "source": [
        "!cd edgefusion-rt/ && python scripts/baseline_bench.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
